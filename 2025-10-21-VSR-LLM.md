---
---
## LLMが読唇術？視覚音声認識最前線

- 小笠原寛明 @ 松尾研LLMコミュニティ【Paper&Hacks#62】

---
layout: two-cols-header
---

# 目次

::left::

- 自己紹介
- 読唇術の研究とは？
  - 視覚音声認識
  - 視覚音声認識の応用
- デモ
- なぜLLMなのか？
  - 認識精度
  - Viseme

::right::

- 単なるマルチモーダルLLMとは違うのか？
  - 画像に対応したLLMとは違うのか？
  - 音声に対応したLLMとは違うのか？
  - 動画に対応したLLMとは違うのか？
- 視覚音声認識LLMの研究
  - VSP-LLM
  - Personalized Lip Reading
  - Zero-AVSR
- まとめ

<!-- 
TODO: 
- それぞれの論文の被引用数を見る
- MMS-LLaMA を加える
-->

---
src: ./pages/intro-hiroga.md
---
---
layout: center
---

# 読唇術の研究とは？

---

# 視覚音声認識

TODO

<!-- 

VSRという
WER, CERを測る

 -->

---

# 視覚音声認識の応用

TODO

---
layout: center
---

# デモ

---

TODO

---
layout: center
---

# なぜLLMなのか？

---

# 認識精度

視覚音声認識 (VSR) の精度は、音声認識 (ASR) や 視覚音声・音声認識 (AVSR) に比べて低い。

TODO: WERやCERを比較した表

---

TODO: WERやCERの解説

---

TODO: whisperの例

---

# Viseme

<!-- TODO: 共調? についても触れる -->

---
layout: center
---

# 単なるマルチモーダルLLMとは違うのか？

---

# 画像に対応したLLMとは違うのか？

---

# 音声に対応したLLMとは違うのか？

---

# 動画に対応したLLMとは違うのか？

---
layout: center
---

# 視覚音声認識LLMの研究

---

# 視覚音声認識LLMの研究

視覚音声認識LLMの研究の一部を表にまとめました。

| 論文 | 特徴 |
|---|---|
| VSP-LLM |  |
| Personalized Lip Reading |  |
| Zero-AVSR |  |
| MMS-LLaMA |  |

<!-- TODO: Short titleであることを断る -->

---
layout: center
---

# Where Visual Speech Meets Language: VSP-LLM Framework for Efficient and Context-Aware Visual Speech Processing
[^VSPLLM]
[^VSPLLM]: [J. H. Yeo et al., “Where Visual Speech Meets Language: VSP-LLM Framework for Efficient and Context-Aware Visual Speech Processing,” May 14, 2024, arXiv: arXiv:2402.15151. doi: 10.48550/arXiv.2402.15151.](https://arxiv.org/abs/2402.15151)

---

# VSP-LLM: 問題設定

---

# VSP-LLM: 貢献

---

# VSP-LLM: 手法

---

# VSP-LLM: 実験

---

# VSP-LLM: 考察

- LLMそのものの性能の差を強く感じました
  - Llama2なので、暴走して同じ単語がずっと繰り返されることもあります

---
layout: center
---

# Personalized Lip Reading: Adapting to Your Unique Lip Movements with Vision and Language[^PersonalizedLipReading]
[^PersonalizedLipReading]: [J. H. Yeo et al., “Personalized Lip Reading: Adapting to Your Unique Lip Movements with Vision and Language,” Jan. 01, 2025, arXiv: arXiv:2409.00986. doi: 10.48550/arXiv.2409.00986.](https://arxiv.org/abs/2409.00986)

---

# Personalized Lip Reading: 貢献

---

# Personalized Lip Reading: 問題設定

---

# Personalized Lip Reading: 貢献

---

# Personalized Lip Reading: 手法

---
layout: center
---

# Zero-AVSR: Zero-Shot Audio-Visual Speech Recognition with LLMs by Learning Language-Agnostic Speech Representations[^ZeroAVSR]
[^ZeroAVSR]: [J. H. Yeo, M. Kim, C. W. Kim, S. Petridis, and Y. M. Ro, “Zero-AVSR: Zero-Shot Audio-Visual Speech Recognition with LLMs by Learning Language-Agnostic Speech Representations,” July 21, 2025, arXiv: arXiv:2503.06273. doi: 10.48550/arXiv.2503.06273.](https://arxiv.org/abs/2503.06273)

---

# Zero-AVSR: 問題設定

- 従来の音声・視覚認識モデルは、学習した言語に依存している
- 従来の多言語音声・視覚認識のデータセットは、わずかな種類の言語にしか対応していない

---

mTEDx: 英語以外の8言語に対応

<a href="https://www.openslr.org/100" target="_blank"><img class="h-100 place-self-center" src="/vsr-llm/openslr-mtedx.png"></a>[^mTEDx]

[^mTEDx]: [E. Salesky et al., “The Multilingual TEDx Corpus for Speech Recognition and Translation,”](https://arxiv.org/abs/2102.01757)

---

MuAViC: 英語を含む9言語に対応。LRS3やmTEDxなどからなる。

<a href="https://github.com/facebookresearch/muavic" target="_blank"><img class="h-80 place-self-center" src="/vsr-llm/github-muavic.png"></a>[^MuAViC]

[^MuAViC]: [M. Anwar et al., “MuAViC: A Multilingual Audio-Visual Corpus for Robust Speech Recognition and Robust Speech-to-Text Translation,”](https://github.com/facebookresearch/muavic)

---

# Zero-AVSR: 貢献

---

# Zero-AVSR: 手法

---

# Zero-AVSR: 手法

- infer.py$$
  - 推論用のオブジェクトを手続き的に呼ぶ実装
    - どのようにromanizerを内包している？: `self.models: []`内部に持っている
    - modelはfairseqでいい感じにロードしている...まさかfairseqのモデルってクラス情報も重みの中に持っているのか？ → 持ってる。
    - 具体的には `@register_model("av_romanizer"` で登録したモデルについて、 cfg.model._name から読み込んでいる
    - infer.pyで扱っているモデルは `av-romanizer/all/checkpoint_best.pt`
  - process_sample(): 肝になる処理は models[0].extract_features
    - AV_Romanizerの self.avromanizer が処理しているらしい...どれだけネストしてるの？
      - FairseqEncoderのサブクラスとしてAVRomanizerWrapperが使われている
      - そこでも実際の処理は w2v_model が担当していて...
        - w2v_model は avhubertのクラスらしい
  - merge_shards(): シャードとは

もしかしてword単位で分かれていない言語についてサポートしていないのか？

---

# Zero-AVSR: 実験

---

# Zero-AVSR: 考察

---

# まとめ

---

# Reference

- [J. H. Yeo et al., “Where Visual Speech Meets Language: VSP-LLM Framework for Efficient and Context-Aware Visual Speech Processing,” May 14, 2024, arXiv: arXiv:2402.15151. doi: 10.48550/arXiv.2402.15151.](https://arxiv.org/abs/2402.15151)
- [J. H. Yeo et al., “Personalized Lip Reading: Adapting to Your Unique Lip Movements with Vision and Language,” Jan. 01, 2025, arXiv: arXiv:2409.00986. doi: 10.48550/arXiv.2409.00986.](https://arxiv.org/abs/2409.00986)
- [J. H. Yeo et al., “Zero-AVSR: Zero-Shot Audio-Visual Speech Recognition with LLMs by Learning Language-Agnostic Speech Representations,” July 21, 2025, arXiv: arXiv:2503.06273. doi: 10.48550/arXiv.2503.06273.](https://arxiv.org/abs/2503.06273)

- E. Salesky et al., “The Multilingual TEDx Corpus for Speech Recognition and Translation,” June 15, 2021, arXiv: arXiv:2102.01757. doi: 10.48550/arXiv.2102.01757.
- M. Anwar et al., “MuAViC: A Multilingual Audio-Visual Corpus for Robust Speech Recognition and Robust Speech-to-Text Translation,” Mar. 07, 2023, arXiv: arXiv:2303.00628. doi: 10.48550/arXiv.2303.00628.

- [K. Rezaee and M. Yeganeh, “Automatic Visual Lip Reading: A Comparative Review of Machine-Learning Approaches,” Results in Engineering, p. 107171, Sept. 2025, doi: 10.1016/j.rineng.2025.107171.](https://www.sciencedirect.com/science/article/pii/S2590123025032268)

