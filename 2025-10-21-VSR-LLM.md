---
---
## LLMが読唇術？視覚音声認識最前線

- 小笠原寛明 @ 松尾研LLMコミュニティ【Paper&Hacks#62】

---
src: ./pages/intro-hiroga.md
---

---
layout: two-cols-header
---

## 目次

::left::

- デモ
- 読唇術の研究とは？
  - 視覚音声認識
  - 視覚音声認識の応用
- なぜLLMなのか？
  - 認識精度
  - Homophone / Viseme

::right::

- 単なるマルチモーダルLLMとは違うのか？
  - 画像に対応したLLMとは違うのか？
  - 音声に対応したLLMとは違うのか？
  - 動画に対応したLLMとは違うのか？
- 視覚音声認識LLMの研究
  - VSP-LLM
  - Personalized Lip Reading
  - Zero-AVSR
- まとめ

<!-- 
TODO: 
- それぞれの論文の被引用数を見る
- MMS-LLaMA を加える
-->

---
layout: center
---

# 読唇術の研究とは？

---

## 視覚音声認識

TODO

<!-- 

VSRという
WER, CERを測る

 -->

---

## 視覚音声認識の応用

TODO

---
layout: center
---

## デモ

---

TODO

---
layout: center
---

# なぜLLMなのか？

---

## 認識精度

視覚音声認識 (VSR) の精度は、音声認識 (ASR) や 視覚音声・音声認識 (AVSR) に比べて低い。

TODO: WERやCERを比較した表

---

TODO: WERやCERの解説

---

TODO: whisperの例

---

## Homophone / Viseme

<!-- TODO: 共調? についても触れる -->

---
layout: center
---

## 単なるマルチモーダルLLMとは違うのか？

---

## 画像に対応したLLMとは違うのか？

---

## 音声に対応したLLMとは違うのか？

---

## 動画に対応したLLMとは違うのか？

---
layout: center
---

# 視覚音声認識LLMの研究

---

## 視覚音声認識LLMの研究

視覚音声認識LLMの研究の一部を表にまとめました。

| 論文 | 特徴 |
|---|---|
| VSP-LLM |  |
| Personalized Lip Reading |  |
| Zero-AVSR |  |
| MMS-LLaMA |  |

<!-- TODO: Short titleであることを断る -->

---
layout: center
---

# Where Visual Speech Meets Language: VSP-LLM Framework for Efficient and Context-Aware Visual Speech Processing
[^VSPLLM]
[^VSPLLM]: [J. H. Yeo et al., “Where Visual Speech Meets Language: VSP-LLM Framework for Efficient and Context-Aware Visual Speech Processing,” May 14, 2024, arXiv: arXiv:2402.15151. doi: 10.48550/arXiv.2402.15151.](https://arxiv.org/abs/2402.15151)

---

## VSP-LLM: 問題設定

- LLMの能力をVSRに活かすことはできないのか？
- 訓練データがわずかでも、LLMの能力を使えば十分な性能を発揮できるか？

---

## VSP-LLM: 貢献

1. 視覚的音声モデリングをLLMと統合し、VSRとVSTで最先端のパフォーマンスを達成した最初の研究
2. VSRとVSTの両方を単一の訓練済みモデルで実行できる、統合された視覚的音声処理モデルを開発した最初の研究
3. 計算効率を大幅に向上させる新しい視覚的音声重複排除手法
4. 提案されたVSP-LLMが、わずか15時間のラベル付きデータという限られた訓練リソースの状況でも、最近の翻訳モデル Anwar et al. (2023) を上回る優れたパフォーマンスで多機能を実行できる

---

## VSP-LLM: 手法

<img class="h-80 place-self-center" src="/vsr-llm/vsp-llm-x1.png">[^VSP-LLM]
[^VSP-LLM]: [J. H. Yeo et al., “Where Visual Speech Meets Language: VSP-LLM Framework for Efficient and Context-Aware Visual Speech Processing,”](https://arxiv.org/abs/2402.15151)

---

## VSP-LLM: 擬似コード

映像の特徴, クラスタIDごとの連続するフレーム数, LLMへのインストラクションがモデルへの入力になっています

```py
vsp_llm.generate({
  "source": {
    "audio": None,
    "video": torch.Tensor,
    "cluster_counts": [1, 3, 2, 1, 4, 1, 1, 1, 3, 1, 2],
    "text": some_instruction,
  },
  "padding_mask": torch.Tensor,
  "text_attn_mask": torch.Tensor,
})
```
<!-- TODO: padding_mask, text_attn_mask についても解説 -->

---

## AV-HuBERTが出力する埋め込みを眺める

<img class="h-80 place-self-center" src="https://img.youtube.com/vi/3y3JY4iEamc/maxresdefault.jpg">[^AV-HuBERT]
[^AV-HuBERT]: [B. Shi et al., “Learning Audio-Visual Speech Representation by Masked Multimodal Cluster Prediction,”](https://arxiv.org/abs/2201.02184) を元に [@xhirogaが開発](https://www.youtube.com/watch?v=3y3JY4iEamc)

---

VSP-LLMと他のVSR手法の比較。LLMを用いたことで、自己教師あり学習のモデルでは最高性能。教師あり学習と比較しても平均的性能。

<img class="h-80 place-self-center" src="/vsr-llm/vsp-llm-table1.png">[^VSP-LLM]

[^VSP-LLM]: [J. H. Yeo et al., “Where Visual Speech Meets Language: VSP-LLM Framework for Efficient and Context-Aware Visual Speech Processing,”](https://arxiv.org/abs/2402.15151)

<!-- VST性能については省略 -->

<!-- 実装詳細についてもできれば次のスライドで触れる -->

---

## VSP-LLM: デモ

---

## VSP-LLM: 考察

- LLMそのものの性能の差を強く感じました
  - Llama2なので、暴走して同じ単語がずっと繰り返されることもあります
- AV-HuBERTの埋め込みをLLMに入力する際、「クラスタリング + 平均の計算」は二度手間に感じます
  - クラスタリングできるくらい似ているなら、平均を計算する必要がないのでは？
  - 実際、後続の研究では、事前に計算した代表値を用いることがあります

---
layout: center
---

# Personalized Lip Reading: Adapting to Your Unique Lip Movements with Vision and Language[^PersonalizedLipReading]
[^PersonalizedLipReading]: [J. H. Yeo et al., “Personalized Lip Reading: Adapting to Your Unique Lip Movements with Vision and Language,” Jan. 01, 2025, arXiv: arXiv:2409.00986. doi: 10.48550/arXiv.2409.00986.](https://arxiv.org/abs/2409.00986)

---

# Personalized Lip Reading: 貢献

---

# Personalized Lip Reading: 問題設定

---

# Personalized Lip Reading: 貢献

---

# Personalized Lip Reading: 手法

---
layout: center
---

# Zero-AVSR: Zero-Shot Audio-Visual Speech Recognition with LLMs by Learning Language-Agnostic Speech Representations[^ZeroAVSR]
[^ZeroAVSR]: [J. H. Yeo, M. Kim, C. W. Kim, S. Petridis, and Y. M. Ro, “Zero-AVSR: Zero-Shot Audio-Visual Speech Recognition with LLMs by Learning Language-Agnostic Speech Representations,” July 21, 2025, arXiv: arXiv:2503.06273. doi: 10.48550/arXiv.2503.06273.](https://arxiv.org/abs/2503.06273)

---

## Zero-AVSR: 問題設定

- 従来の音声・視覚認識モデルは、学習した言語に依存している
- 従来の多言語音声・視覚認識のデータセットは、わずかな種類の言語にしか対応していない

---

mTEDx: 英語以外の8言語に対応

<a href="https://www.openslr.org/100" target="_blank"><img class="h-100 place-self-center" src="/vsr-llm/openslr-mtedx.png"></a>[^mTEDx]

[^mTEDx]: [E. Salesky et al., “The Multilingual TEDx Corpus for Speech Recognition and Translation,”](https://arxiv.org/abs/2102.01757)

---

MuAViC: 英語を含む9言語に対応。LRS3やmTEDxなどからなる。

<a href="https://github.com/facebookresearch/muavic" target="_blank"><img class="h-80 place-self-center" src="/vsr-llm/github-muavic.png"></a>[^MuAViC]

[^MuAViC]: [M. Anwar et al., “MuAViC: A Multilingual Audio-Visual Corpus for Robust Speech Recognition and Robust Speech-to-Text Translation,”](https://github.com/facebookresearch/muavic)

<!-- TODO: なぜ train hours 表記なんだっけ？補足する -->

---

## Zero-AVSR: 貢献

- MARCデータセットの提案
- AV-HuBERT & Llamaのアーキテクチャにおける言語間の転移学習のポテンシャル実証
- ローマ字を挟むことで可視性能を上げる工夫

---
layout: two-cols-header
---

## Zero-AVSR: 手法 (MARCデータセット)

::left::

### 元になるデータ

- LRS3(433時間, 英語, ラベルあり)
- MuAViC(1,200時間, 9言語, ラベルあり)
- VoxCeleb2(2,442時間, 多言語, ラベルなし)
- AVSpeech(4,700時間, 多言語, ラベルなし)

::right::

### ローマ字化した文字起こしを提案

- der puls und der blutdruck steigen → d e r | p u l s | u n d | d e r | b l u t d r u c k | s t e i g e n |
- vielen dank → v i e l e n | d a n k |

---

## Zero-AVSR: 手法 (Zero-AVSR)

<img class="h-80 place-self-center" src="/vsr-llm/zero-avsr-x1.png">[^Zero-AVSR]

[^Zero-AVSR]: [J. H. Yeo, M. Kim, C. W. Kim, S. Petridis, and Y. M. Ro, “Zero-AVSR: Zero-Shot Audio-Visual Speech Recognition with LLMs by Learning Language-Agnostic Speech Representations,”](https://arxiv.org/abs/2503.06273)

---

## Zero-AVSR: 手法 (Zero-AVSR)

<img class="h-80 place-self-center" src="/vsr-llm/zero-avsr-x2.png">[^Zero-AVSR]

[^Zero-AVSR]: [J. H. Yeo, M. Kim, C. W. Kim, S. Petridis, and Y. M. Ro, “Zero-AVSR: Zero-Shot Audio-Visual Speech Recognition with LLMs by Learning Language-Agnostic Speech Representations,”](https://arxiv.org/abs/2503.06273)

---

# Zero-AVSR: 実験

---

# Zero-AVSR: 考察

---

# Zero-AVSR: もっと知りたい

https://zenn.dev/hiroga/articles/zero-avsr-eval

---

# まとめ

---

# Reference

- [J. H. Yeo et al., “Where Visual Speech Meets Language: VSP-LLM Framework for Efficient and Context-Aware Visual Speech Processing,” May 14, 2024, arXiv: arXiv:2402.15151. doi: 10.48550/arXiv.2402.15151.](https://arxiv.org/abs/2402.15151)
- [J. H. Yeo et al., “Personalized Lip Reading: Adapting to Your Unique Lip Movements with Vision and Language,” Jan. 01, 2025, arXiv: arXiv:2409.00986. doi: 10.48550/arXiv.2409.00986.](https://arxiv.org/abs/2409.00986)
- [J. H. Yeo et al., “Zero-AVSR: Zero-Shot Audio-Visual Speech Recognition with LLMs by Learning Language-Agnostic Speech Representations,” July 21, 2025, arXiv: arXiv:2503.06273. doi: 10.48550/arXiv.2503.06273.](https://arxiv.org/abs/2503.06273)

- [E. Salesky et al., “The Multilingual TEDx Corpus for Speech Recognition and Translation,” June 15, 2021, arXiv: arXiv:2102.01757. doi: 10.48550/arXiv.2102.01757.](https://arxiv.org/abs/2102.01757)
- [M. Anwar et al., “MuAViC: A Multilingual Audio-Visual Corpus for Robust Speech Recognition and Robust Speech-to-Text Translation,” Mar. 07, 2023, arXiv: arXiv:2303.00628. doi: 10.48550/arXiv.2303.00628.](https://arxiv.org/abs/2303.00628)

- [B. Shi et al., “Learning Audio-Visual Speech Representation by Masked Multimodal Cluster Prediction,” Mar. 13, 2022, arXiv: arXiv:2201.02184. doi: 10.48550/arXiv.2201.02184.](https://arxiv.org/abs/2201.02184)

- [K. Rezaee and M. Yeganeh, “Automatic Visual Lip Reading: A Comparative Review of Machine-Learning Approaches,” Results in Engineering, p. 107171, Sept. 2025, doi: 10.1016/j.rineng.2025.107171.](https://www.sciencedirect.com/science/article/pii/S2590123025032268)

