---
summary: この資料では、視覚音声認識（VSR）やその関連研究の概要、特に大規模言語モデル（LLM）を用いた新たなアプローチについて説明します。デモを通じて、VSP-LLMやZero-AVSRといったモデルの具体的な性能や応用例を紹介し、従来の手法との比較や課題についても言及します。また、データセット作成の難しさや従来モデルへの依存度など、視覚情報の認識における現状についても考察します。
---
# LLMが読唇術？視覚音声認識
小笠原寛明 @ 松尾研LLMコミュニティ[【Paper&Hacks#62】](https://matsuolab-community.connpass.com/event/372877/)
- [J. H. Yeo et al., “Where Visual Speech Meets Language: VSP-LLM Framework for Efficient and Context-Aware Visual Speech Processing,”](https://arxiv.org/abs/2402.15151)
- [J. H. Yeo et al., “Zero-AVSR: Zero-Shot Audio-Visual Speech Recognition with LLMs by Learning Language-Agnostic Speech Representations,”](https://arxiv.org/abs/2503.06273)

<span class="absolute bottom-6 left-0 right-0 text-center text-xs text-slate-400">
  この資料を
  <a class="text-slate-300 underline decoration-dotted" href="https://github.com/xhiroga/talks/blob/main/2025-10-21-VSR-LLM.md" target="_blank"><img src="/github-mark.svg" alt="GitHub mark" class="inline-block h-4 w-4" /> GitHubで公開</a>
  しています。
</span>


---
src: ./pages/intro-hiroga.md
---
---

## 目次

- デモ
- 読唇術の研究とは？
  - 視覚音声認識
  - 視覚音声認識の応用
- なぜLLMなのか？
  - Homophone / Viseme
  - データセット作成の難しさ
  - 単なるマルチモーダルLLMとは違うのか？
- 視覚音声認識LLMの研究
  - VSP-LLM
  - Zero-AVSR
- まとめ

<!-- 
TODO: 
- それぞれの論文の被引用数を見る
- MMS-LLaMA を加える
-->

---
layout: center
---

# デモ

---

## VSP-LLM: デモ (WebCam)

<img class="h-80 place-self-center" src="https://img.youtube.com/vi/ltW1nRTGCH0/maxresdefault.jpg">[^VSP-LLM]

[^VSP-LLM]: [Sally-SH/VSP-LLM](https://github.com/Sally-SH/VSP-LLM) を元に [@xhirogaが開発](https://www.youtube.com/watch?v=ltW1nRTGCH0)

---
layout: center
---

# 読唇術の研究とは？

---

## 視覚音声認識

VSR (Visual Speech Recognition, 視覚音声認識) または V-ASR (Visual Automatic Speech Recognition) と呼ばれる。

発話の映像のみを元に、発話内容を文字起こしするタスクを解く研究。

文字起こしした結果のWER (Word Error Rate, 単語誤り率) や CER (Character Error Rate, 文字誤り率)が**低いほど良い**

関連する研究分野に、VST (Visual Speech Translation), ASR (Automatic Speech Recognition), AVSR (Audio-Visual Speech Recognition)などがある。

---

VSRモデルの比較。多くのモデルで誤り率が25%以上であること、大量データが性能改善に効くこと、LLMを用いたモデル（Ours = VALLR）は追加学習がわずかでも性能が高いことが分かる。[^VALLR]

<div class="grid grid-cols-2">
  <img class="h-80 place-self-center" src="/vsr-llm/vallr-table2-1.png" alt="VALLR Table 2 (Part 1)">
  <img class="h-80 place-self-center" src="/vsr-llm/vallr-table2-2.png" alt="VALLR Table 2 (Part 2)">
</div>

[^VALLR]: [M. Thomas et al., “VALLR: Visual ASR Language Model for Lip Reading,” Mar. 27, 2025, arXiv: arXiv:2503.21408. doi: 10.48550/arXiv.2503.21408.](https://arxiv.org/abs/2503.21408v1)

---

ASRの最先端モデルは軒並みWER10%を切っている ＝ **視覚情報のみから推測するVSRは難しい！**

<img class="h-80 place-self-center" src="/vsr-llm/open_asr_leaderboard.png">[^open_asr_leaderboard]

[^open_asr_leaderboard]: [Srivastav et al., "Open Automatic Speech Recognition Leaderboard"](https://huggingface.co/spaces/hf-audio/open_asr_leaderboard)

---

## 視覚音声認識の応用

- 甲状腺がんや咽頭がんなどによって声を失ってしまった方の発話のサポート
- 図書館のように静かな場所や、工事現場のように騒がしい場所での発話認識

---

## 視覚音声認識スタートアップ: Liopa

- 医療現場に対して、限定された語彙から90%以上の精度で視覚音声認識を行うサービスを展開していた
- 2025年現在、Webサイトはクローズ状態

<img class="h-80 place-self-center" src="/vsr-llm/liopa.png">

---

## 音声認識スタートアップ: Whispp

- ささやき声を補強して伝えるアプリを展開するスタートアップ
- 吃音を持つ方などでも、ささやき声であればリラックスできることがある、といった点に着目

<img class="h-60 place-self-center" src="/vsr-llm/whispp.png">[^whispp]

[^whispp]: https://whispp.com/

---
layout: center
---

# なぜLLMなのか？

---

## Homophone / Viseme

口の形が同じでも、異なる音のことを Homophone (ホモフォン, 同口形異音) といいます。例えば、「p」と「m」は口の形が同じです。

<img class="h-40 place-self-center" src="/vsr-llm/meta-viseme.png">[^meta]

[^meta]: [Meta, "口形素リファレンス."](https://developers.meta.com/horizon/documentation/unity/audio-ovrlipsync-viseme-reference/?locale=ja_JP)

人間が言葉を話すときの口の形を、IPA（国際発音記号）のように分類したものをViseme（ビゼーム, 口形素, 視覚音素）と言います。ビゼームの数は15種類とされることが多いですが、より細かく分類する場合もあります。

<!-- TODO: 共調? についても触れる -->

---

## データセット作成の難しさ

読唇タスクのためのデータセットの作成には、音声認識のためのデータセット作成とは異なる難しさがあります。

- 映像データより音声データの方がファイルサイズが大きい場合が多い
- 映像データでは収録時に顔や部屋などが映ってしまうため、プライバシーの問題がより懸念される

一方、収録された音声にノイズが入っている場合などは、用途によっては許容できる場合もあります。

---

## 単なるマルチモーダルLLMとは違うのか？: 実験

<img class="h-80 place-self-center" src="https://img.youtube.com/vi/x3gQvi3adng/maxresdefault.jpg">[^hiroga]

[^hiroga]: [Try lip reading by Gemini 2.5 Pro (0/3)](https://www.youtube.com/watch?v=x3gQvi3adng)

---

## 単なるマルチモーダルLLMとは違うのか？: 回答

マルチモーダルLLMにもVSRタスクのための土台はあるが、そのままでは困難。

- 映像を3次元畳み込みしてから埋め込みを出力するという、大まかな構成はそのまま使える
- 実際には、口の形と音素（またはビゼーム）が一致するように学習する必要がある
- 既存のマルチモーダルLLMを追加学習してVSRに転用する研究はまだほとんどない

したがって、VSRの研究では音声認識の事前学習済みモデル（＋LLMなど文字起こしバックエンド）を用いることが多い！

後ほど視覚音声認識モデルである**AV-HuBERT**のデモをします。

---

## なぜLLMなのか？

数千〜数万時間の学習が必要な既存手法と比較して、LLMの知識を活かすことで1/10以下の学習時間に抑えることが可能と見込まれるため。[^VALLR]

<div class="grid grid-cols-2">
  <img class="h-80 place-self-center" src="/vsr-llm/vallr-table2-1.png" alt="VALLR Table 2 (Part 1)">
  <img class="h-80 place-self-center" src="/vsr-llm/vallr-table2-2.png" alt="VALLR Table 2 (Part 2)">
</div>

[^VALLR]: [M. Thomas et al., “VALLR: Visual ASR Language Model for Lip Reading,” Mar. 27, 2025, arXiv: arXiv:2503.21408. doi: 10.48550/arXiv.2503.21408.](https://arxiv.org/abs/2503.21408v1)


---
layout: center
---

# 視覚音声認識LLMの研究

---

<div class="text-4">

視覚音声認識LLMの研究の一部を表にまとめました。VSRとAVSRのどちらもあります。⭐️マークが本日扱う論文です。[^references]

| 論文 | 公開年月 | 種類 | 特徴 |
|---|---|---|---|
| VSP-LLM⭐️ | 2024-05 | VSR | AV-HuBERTとLlamaを初めて接続 |
| Personalized Lip Reading | 2024-09 | VSR | 自前のエンコーダーを話者別にLoRA可能に |
| Llama-AVSR | 2024-09 | AVSR | 音声と視覚それぞれの特徴をトークン化しLlamaに入力 |
| Zero-AVSR⭐️ | 2025-03 | AVSR | ローマ字を経由することで制御しやすく |
| MMS-LLaMA | 2025-03 | AVSR | 視覚音声の特徴を合成後にトークン化することで計算量削減 |
| VALLR | 2025-03 | VSR | 自前エンコーダーから音素を予測しLlamaに入力 |
| PV-ASR | 2025-07 | VSR | 自前エンコーダーと口ランドマークの特徴を合成 |

[^references]: スライド末尾「参考文献」を参照ください。

</div>

---
layout: center
---

# Where Visual Speech Meets Language: VSP-LLM Framework for Efficient and Context-Aware Visual Speech Processing
[^VSPLLM]
[^VSPLLM]: [J. H. Yeo et al., “Where Visual Speech Meets Language: VSP-LLM Framework for Efficient and Context-Aware Visual Speech Processing,” May 14, 2024, arXiv: arXiv:2402.15151. doi: 10.48550/arXiv.2402.15151.](https://arxiv.org/abs/2402.15151)

---

## VSP-LLM: 問題設定

- LLMの能力をVSRに活かすことはできないのか？
- 訓練データがわずかでも、LLMの能力を使えば十分な性能を発揮できるか？

---

## VSP-LLM: VSRにおける貢献

1. 視覚的音声モデリングをLLMと統合し、VSRとVSTで最先端のパフォーマンスを達成した最初の研究
2. 連続するフレームをまとめるにあたって、機械的に間隔を設けるのではなく、埋め込みの特徴に応じて k-means でクラスタリング

---

## VSP-LLM: 手法

<img class="h-80 place-self-center" src="/vsr-llm/vsp-llm-x1.png">[^VSP-LLM]

[^VSP-LLM]: [J. H. Yeo et al., “Where Visual Speech Meets Language: VSP-LLM Framework for Efficient and Context-Aware Visual Speech Processing,”](https://arxiv.org/abs/2402.15151)

---

## VSP-LLM: 擬似コード

映像の特徴, クラスタIDごとの連続するフレーム数, LLMへのインストラクションがモデルへの入力になっています

```py
vsp_llm.generate({
  "source": {
    "audio": None,
    "video": torch.Tensor,
    "cluster_counts": [1, 3, 2, 1, 4, 1, 1, 1, 3, 1, 2],
    "text": some_instruction,
  },
  "padding_mask": torch.Tensor,
  "text_attn_mask": torch.Tensor,
})
```
<!-- TODO: padding_mask, text_attn_mask についても解説 -->

---

## AV-HuBERTが出力する埋め込みを眺める

<img class="h-80 place-self-center" src="https://img.youtube.com/vi/3y3JY4iEamc/maxresdefault.jpg">[^AV-HuBERT]

[^AV-HuBERT]: [B. Shi et al., “Learning Audio-Visual Speech Representation by Masked Multimodal Cluster Prediction,”](https://arxiv.org/abs/2201.02184) を元に [@xhirogaが開発](https://www.youtube.com/watch?v=3y3JY4iEamc)

---

VSP-LLMと他のVSR手法の比較。LLMを用いたことで、自己教師あり学習のモデルでは最高性能。教師あり学習と比較しても平均的性能。

<img class="h-80 place-self-center" src="/vsr-llm/vsp-llm-table1.png">[^VSP-LLM]

[^VSP-LLM]: [J. H. Yeo et al., “Where Visual Speech Meets Language: VSP-LLM Framework for Efficient and Context-Aware Visual Speech Processing,”](https://arxiv.org/abs/2402.15151)

<!-- VST性能については省略 -->

<!-- 実装詳細についてもできれば次のスライドで触れる -->

---

## VSP-LLM: デモ (LRS3)

<img class="h-80 place-self-center" src="https://img.youtube.com/vi/xN2htGRxC-M/maxresdefault.jpg">[^VSP-LLM]

[^VSP-LLM]: [Sally-SH/VSP-LLM](https://github.com/Sally-SH/VSP-LLM) を元に [@xhirogaが開発](https://www.youtube.com/watch?v=xN2htGRxC-M)

---

## 【再掲】VSP-LLM: デモ (WebCam)

<img class="h-80 place-self-center" src="https://img.youtube.com/vi/ltW1nRTGCH0/maxresdefault.jpg">[^VSP-LLM]

[^VSP-LLM]: [Sally-SH/VSP-LLM](https://github.com/Sally-SH/VSP-LLM) を元に [@xhirogaが開発](https://www.youtube.com/watch?v=ltW1nRTGCH0)

---

## VSP-LLM: 考察

- LLMそのものの性能の差を強く感じました
  - Llama2なので、暴走して同じ単語がずっと繰り返されることもあります
- AV-HuBERTの埋め込みをLLMに入力する際、「クラスタリング + 平均の計算」は二度手間に感じます
  - クラスタリングできるくらい似ているなら、平均を計算する必要がないのでは？
  - 実際、後続の研究では、事前に計算した代表値を用いることがあります
- 標準的なデータセットでの性能が高い一方で、Webカメラでの撮影など現実世界での利用にはまだまだ弱そうです

---
layout: center
---

# Zero-AVSR: Zero-Shot Audio-Visual Speech Recognition with LLMs by Learning Language-Agnostic Speech Representations[^ZeroAVSR]
[^ZeroAVSR]: [J. H. Yeo, M. Kim, C. W. Kim, S. Petridis, and Y. M. Ro, “Zero-AVSR: Zero-Shot Audio-Visual Speech Recognition with LLMs by Learning Language-Agnostic Speech Representations,” July 21, 2025, arXiv: arXiv:2503.06273. doi: 10.48550/arXiv.2503.06273.](https://arxiv.org/abs/2503.06273)

---

## Zero-AVSR: 問題設定

- 従来の音声・視覚認識モデルは、学習した言語に依存している
- 従来の多言語音声・視覚認識のデータセットは、わずかな種類の言語にしか対応していない

---

mTEDx: 英語以外の8言語に対応

<a href="https://www.openslr.org/100" target="_blank"><img class="h-100 place-self-center" src="/vsr-llm/openslr-mtedx.png"></a>[^mTEDx]

[^mTEDx]: [E. Salesky et al., “The Multilingual TEDx Corpus for Speech Recognition and Translation,”](https://arxiv.org/abs/2102.01757)

---

MuAViC: 英語を含む9言語に対応。LRS3やmTEDxなどからなる。

<a href="https://github.com/facebookresearch/muavic" target="_blank"><img class="h-80 place-self-center" src="/vsr-llm/github-muavic.png"></a>[^MuAViC]

[^MuAViC]: [M. Anwar et al., “MuAViC: A Multilingual Audio-Visual Corpus for Robust Speech Recognition and Robust Speech-to-Text Translation,”](https://github.com/facebookresearch/muavic)

<!-- TODO: なぜ train hours 表記なんだっけ？補足する -->

---

## Zero-AVSR: 貢献

- MARCデータセットの提案
- AV-HuBERT & Llamaのアーキテクチャにおける言語間の転移学習のポテンシャル実証
- ローマ字を挟むことで可視性能を上げる工夫

---
layout: two-cols-header
---

## Zero-AVSR: 手法 (MARCデータセット)

::left::

### 元になるデータ

- LRS3(433時間, 英語, ラベルあり)
- MuAViC(1,200時間, 9言語, ラベルあり)
- VoxCeleb2(2,442時間, 多言語, ラベルなし)
- AVSpeech(4,700時間, 多言語, ラベルなし)

::right::

### ローマ字化した文字起こしを提案

- der puls und der blutdruck steigen → d e r | p u l s | u n d | d e r | b l u t d r u c k | s t e i g e n |
- vielen dank → v i e l e n | d a n k |

---

## Zero-AVSR: 手法 (Cascaded Zero-AVSR)

<img class="h-80 place-self-center" src="/vsr-llm/zero-avsr-x1.png">[^Zero-AVSR]

[^Zero-AVSR]: [J. H. Yeo, M. Kim, C. W. Kim, S. Petridis, and Y. M. Ro, “Zero-AVSR: Zero-Shot Audio-Visual Speech Recognition with LLMs by Learning Language-Agnostic Speech Representations,”](https://arxiv.org/abs/2503.06273)

---

Cascaded Zero-AVSRを実装する前に、利用するLLMを実験で決めています。GPT-4o-miniの性能がLlamaに対して良いことが分かります。

<img class="h-80 place-self-center" src="/vsr-llm/zero-avsr-table1.png">[^Zero-AVSR]

[^Zero-AVSR]: [J. H. Yeo et al., “Zero-AVSR: Zero-Shot Audio-Visual Speech Recognition with LLMs by Learning Language-Agnostic Speech Representations,”](https://arxiv.org/abs/2503.06273)

---

## Zero-AVSR: 手法 (Directly Integrated Zero-AVSR)

<img class="h-80 place-self-center" src="/vsr-llm/zero-avsr-x2.png">[^Zero-AVSR]

[^Zero-AVSR]: [J. H. Yeo et al., “Zero-AVSR: Zero-Shot Audio-Visual Speech Recognition with LLMs by Learning Language-Agnostic Speech Representations,”](https://arxiv.org/abs/2503.06273)

---

- Zero-AVSRと複数のAVSRモデルの比較。あらかじめ多言語で訓練されたAVSRモデル（上4つ）と、ターゲット言語での追加学習をしていないとされる（＝ゼロショット）モデル（下3つ）の比較。
- 実際には、提案手法におけるローマ字化モジュールの学習で多言語を用いているほか、Llamaの事前学習でも用いられているはずなので、ゼロショットの定義が気になるが...

<img class="h-60 place-self-center" src="/vsr-llm/zero-avsr-table2.png">[^Zero-AVSR]

[^Zero-AVSR]: [J. H. Yeo et al., “Zero-AVSR: Zero-Shot Audio-Visual Speech Recognition with LLMs by Learning Language-Agnostic Speech Representations,”](https://arxiv.org/abs/2503.06273)

<!-- 実装についても -->

---

## Zero-AVSR: デモ (MuAViC)

<img class="h-80 place-self-center" src="https://img.youtube.com/vi/2jx0UJOOJ_A/maxresdefault.jpg">[^Zero-AVSR]

[^Zero-AVSR]: [JeongHun0716/zero-avsr](https://github.com/JeongHun0716/zero-avsr) を元に [@xhirogaが開発](https://www.youtube.com/watch?v=2jx0UJOOJ_A)

---

## Zero-AVSR: デモ (WebCam)

<img class="h-80 place-self-center" src="https://img.youtube.com/vi/Mcu_1fmdMe4/maxresdefault.jpg">[^Zero-AVSR]

[^Zero-AVSR]: [JeongHun0716/zero-avsr](https://github.com/JeongHun0716/zero-avsr) を元に [@xhirogaが開発](https://www.youtube.com/watch?v=Mcu_1fmdMe4)

---

## Zero-AVSR: 考察

- 引き続き AV-HuBERT と Llama の偉大さがわかる論文
- AV-HuBERTからローマ字を経由してLlamaに入力している...と思わせて、最終的なアーキテクチャでは AV-HuBERTの埋め込みを長さだけ調整してそのまま用いている
  - つまり、そこまでローマ字は関係ない（開発・運用時には確実に便利）
- 個人的には、多言語データセットの学習でAV-HuBERTの既存重みが大きく変わった = 学習時間でスケールする余地がある、という学びの方が多かった

---

## Zero-AVSR: もっと知りたい

https://zenn.dev/hiroga/articles/zero-avsr-eval

---

## まとめ

- 読唇術（視覚音声認識タスク）について、独自アーキテクチャではなくLLMを組み込む手法の開発が進んでいます
- 映像をエンコードするためのさまざまな手法が提案されています
- 音素やローマ字を用いて制御性を上げる工夫も見られます

---

## 参考文献 (VSR/AVSR + LLM) #1

- [J. H. Yeo et al., “Where Visual Speech Meets Language: VSP-LLM Framework for Efficient and Context-Aware Visual Speech Processing,” May 14, 2024, arXiv: arXiv:2402.15151. doi: 10.48550/arXiv.2402.15151.](https://arxiv.org/abs/2402.15151)
- [J. H. Yeo et al., “Personalized Lip Reading: Adapting to Your Unique Lip Movements with Vision and Language,” Jan. 01, 2025, arXiv: arXiv:2409.00986. doi: 10.48550/arXiv.2409.00986.](https://arxiv.org/abs/2409.00986)
- [U. Cappellazzo et al., “Large Language Models are Strong Audio-Visual Speech Recognition Learners,” Mar. 07, 2025, arXiv: arXiv:2409.12319. doi: 10.48550/arXiv.2409.12319.](https://arxiv.org/abs/2409.12319)

---

## 参考文献 (VSR/AVSR + LLM) #2

- [J. H. Yeo et al., “Zero-AVSR: Zero-Shot Audio-Visual Speech Recognition with LLMs by Learning Language-Agnostic Speech Representations,” July 21, 2025, arXiv: arXiv:2503.06273. doi: 10.48550/arXiv.2503.06273.](https://arxiv.org/abs/2503.06273)
- [J. H. Yeo et al., “MMS-LLaMA: Efficient LLM-based Audio-Visual Speech Recognition with Minimal Multimodal Speech Tokens,” June 05, 2025, arXiv: arXiv:2503.11315. doi: 10.48550/arXiv.2503.11315.](https://arxiv.org/abs/2503.11315)
- [M. Thomas et al., “VALLR: Visual ASR Language Model for Lip Reading,” Mar. 27, 2025, arXiv: arXiv:2503.21408. doi: 10.48550/arXiv.2503.21408.](https://arxiv.org/abs/2503.21408v1)
- [M. K. K. Teng et al., “Phoneme-Level Visual Speech Recognition via Point-Visual Fusion and Language Model Reconstruction,” July 25, 2025, arXiv: arXiv:2507.18863. doi: 10.48550/arXiv.2507.18863.](https://arxiv.org/abs/2507.18863)

---

## 参考文献 (Base Model, Datasets, Survey)

- [B. Shi et al., “Learning Audio-Visual Speech Representation by Masked Multimodal Cluster Prediction,” Mar. 13, 2022, arXiv: arXiv:2201.02184. doi: 10.48550/arXiv.2201.02184.](https://arxiv.org/abs/2201.02184)
- [E. Salesky et al., “The Multilingual TEDx Corpus for Speech Recognition and Translation,” June 15, 2021, arXiv: arXiv:2102.01757. doi: 10.48550/arXiv.2102.01757.](https://arxiv.org/abs/2102.01757)
- [M. Anwar et al., “MuAViC: A Multilingual Audio-Visual Corpus for Robust Speech Recognition and Robust Speech-to-Text Translation,” Mar. 07, 2023, arXiv: arXiv:2303.00628. doi: 10.48550/arXiv.2303.00628.](https://arxiv.org/abs/2303.00628)
- [K. Rezaee and M. Yeganeh, “Automatic Visual Lip Reading: A Comparative Review of Machine-Learning Approaches,” Results in Engineering, p. 107171, Sept. 2025, doi: 10.1016/j.rineng.2025.107171.](https://www.sciencedirect.com/science/article/pii/S2590123025032268)
