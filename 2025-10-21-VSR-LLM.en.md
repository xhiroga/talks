---
summary: This document provides an overview of visual speech recognition (VSR) and related research, with a particular focus on novel approaches using large language models (LLMs). Through demonstrations, we will introduce the specific performance and application examples of models such as VSP-LLM and Zero-AVSR, and discuss comparisons with conventional methods and challenges. We will also consider the current state of visual information recognition, including the difficulties in creating datasets and the reliance on conventional models.
---

# Lip Reading with LLMs? Visual Speech Recognition
Hiroaki Ogasawara @ Matsuo Lab LLM Community [Paper&Hacks #62](https://matsuolab-community.connpass.com/event/372877/)
- [J. H. Yeo et al., “Where Visual Speech Meets Language: VSP-LLM Framework for Efficient and Context-Aware Visual Speech Processing,”](https://arxiv.org/abs/2402.15151)
- [J. H. Yeo et al., “Zero-AVSR: Zero-Shot Audio-Visual Speech Recognition with LLMs by Learning Language-Agnostic Speech Representations,”](https://arxiv.org/abs/2503.06273)

<span class="absolute bottom-6 left-0 right-0 text-center text-xs text-slate-400">
  This deck is
  <a class="text-slate-300 underline decoration-dotted" href="https://github.com/xhiroga/talks/blob/main/2025-10-21-VSR-LLM.en.md" target="_blank"><img src="/github-mark.svg" alt="GitHub mark" class="inline-block h-4 w-4" /> available on GitHub</a>.
</span>

---
src: ./pages/intro-hiroga.en.md
---
---

## Agenda

- Demo
- What is lip-reading research?
  - Visual speech recognition
  - Applications of visual speech recognition
- Why LLMs?
  - Homophones / visemes
  - Challenges in building datasets
  - Are they different from general-purpose multimodal LLMs?
- Research on LLM-based visual speech recognition
  - VSP-LLM
  - Zero-AVSR
- Takeaways

<!-- 
TODO:
- Check citation counts for each paper
- Add MMS-LLaMA
-->

---
layout: center
---

# Demo

---

## VSP-LLM: Demo (WebCam)

<img class="h-80 place-self-center" src="https://img.youtube.com/vi/ltW1nRTGCH0/maxresdefault.jpg">[^VSP-LLM]

[^VSP-LLM]: [Sally-SH, “VSP-LLM,” GitHub.](https://github.com/Sally-SH/VSP-LLM) Adapted by [@xhiroga](https://www.youtube.com/watch?v=ltW1nRTGCH0)

---
layout: center
---

# What Is Lip-Reading Research?

---

## Visual Speech Recognition

Also called VSR (Visual Speech Recognition) or V-ASR (Visual Automatic Speech Recognition).

Research that solves the task of transcribing spoken content using only the video of the utterance.

A lower transcription WER (Word Error Rate) or CER (Character Error Rate) means **better** performance.

Related research areas include VST (Visual Speech Translation), ASR (Automatic Speech Recognition), and AVSR (Audio-Visual Speech Recognition).

---

Comparison across VSR models. Many approaches still have an error rate above 25%, large datasets drive performance gains, and the LLM-based model (Ours = VALLR) performs well even with light fine-tuning.[^VALLR]

<div class="grid grid-cols-2">
  <img class="h-80 place-self-center" src="/vsr-llm/vallr-table2-1.png" alt="VALLR Table 2 (Part 1)">
  <img class="h-80 place-self-center" src="/vsr-llm/vallr-table2-2.png" alt="VALLR Table 2 (Part 2)">
</div>

[^VALLR]: [M. Thomas et al., “VALLR: Visual ASR Language Model for Lip Reading,” Mar. 27, 2025, arXiv: arXiv:2503.21408. doi: 10.48550/arXiv.2503.21408.](https://arxiv.org/abs/2503.21408v1)

---

State-of-the-art ASR models already push WER below 10% = **Inferring solely from visual cues in VSR is hard!**

<img class="h-80 place-self-center" src="/vsr-llm/open_asr_leaderboard.png">[^open_asr_leaderboard]

[^open_asr_leaderboard]: [H. Srivastav et al., “Open Automatic Speech Recognition Leaderboard.”](https://huggingface.co/spaces/hf-audio/open_asr_leaderboard)

---

## Applications of Visual Speech Recognition

- Supporting speech for people who have lost their voice due to thyroid or laryngeal cancer
- Recognizing speech in quiet places like libraries or in noisy environments such as construction sites

---

## Visual Speech Recognition Startup: Liopa

- Offered a service for medical settings that achieved over 90% accuracy on a limited vocabulary
- As of 2025, the website is offline

<img class="h-80 place-self-center" src="/vsr-llm/liopa.png">

---

## Speech Technology Startup: Whispp

- A startup building an app that enhances whispered speech so it can be clearly conveyed
- Focuses on the observation that some people with stutters feel more relaxed when whispering

<img class="h-60 place-self-center" src="/vsr-llm/whispp.png">[^whispp]

[^whispp]: [Whispp, “Whispp.”](https://whispp.com/)

---
layout: center
---

# Why LLMs?

---

## Homophones / Visemes

Even when the mouth shape is identical, the produced sounds can differ. Such cases are called homophones. For example, “p” and “m” share the same mouth shape.

<img class="h-40 place-self-center" src="/vsr-llm/meta-viseme.png">[^meta]

[^meta]: [Meta, “Viseme Reference.”](https://developers.meta.com/horizon/documentation/unity/audio-ovrlipsync-viseme-reference/?locale=en_US)

Visemes classify mouth shapes while speaking, analogous to how IPA classifies sounds. Many sources list 15 viseme categories, though finer-grained taxonomies also exist.

<!-- TODO: Add comments on coarticulation -->

---

## Challenges in Building Datasets

Creating datasets for lip-reading tasks involves different hurdles than those for audio ASR.

- Video files are often larger than audio files
- Video captures faces, rooms, and other private information, leading to stronger privacy concerns

Meanwhile, applications can sometimes tolerate noisy audio, depending on use cases.

---

## Are They Different from Generic Multimodal LLMs?: Experiment

<img class="h-80 place-self-center" src="https://img.youtube.com/vi/x3gQvi3adng/maxresdefault.jpg">[^hiroga]

[^hiroga]: [@xhiroga, “Try lip reading by Gemini 2.5 Pro (0/3).”](https://www.youtube.com/watch?v=x3gQvi3adng)

---

## Are They Different from Generic Multimodal LLMs?: Answer

General-purpose multimodal LLMs include the building blocks for VSR, but using them as-is is tough.

- The coarse structure of 3D convolutions followed by embeddings can be reused
- In practice, you must learn to align mouth shapes with phonemes (or visemes)
- Very few studies fine-tune existing multimodal LLMs and repurpose them for VSR

Therefore VSR research often relies on pretrained ASR models (plus LLM-style transcription backends).

We will later demo **AV-HuBERT**, a representative visual speech recognition encoder.

---

## Why LLMs?

Compared with prior approaches that require training for thousands to tens of thousands of hours, leveraging LLM knowledge is expected to cut the training time to under one tenth.

<div class="grid grid-cols-2">
  <img class="h-80 place-self-center" src="/vsr-llm/vallr-table2-1.png" alt="VALLR Table 2 (Part 1)">
  <img class="h-80 place-self-center" src="/vsr-llm/vallr-table2-2.png" alt="VALLR Table 2 (Part 2)">
</div>

[^VALLR]: [M. Thomas et al., “VALLR: Visual ASR Language Model for Lip Reading,” Mar. 27, 2025, arXiv: arXiv:2503.21408. doi: 10.48550/arXiv.2503.21408.](https://arxiv.org/abs/2503.21408v1)

---
layout: center
---

# LLM-Based Visual Speech Recognition Research

---

<div class="text-4">

The table summarizes selected LLM-based visual speech recognition work, covering both VSR and AVSR. ⭐️ marks the papers featured today.[^references]

| Paper | Release Date | Type | Key Idea |
|---|---|---|---|
| VSP-LLM⭐️ | 2024-05 | VSR | First to connect AV-HuBERT with Llama |
| Personalized Lip Reading | 2024-09 | VSR | Enables per-speaker LoRA on a custom encoder |
| Llama-AVSR | 2024-09 | AVSR | Tokenizes audio and visual features before feeding Llama |
| Zero-AVSR⭐️ | 2025-03 | AVSR | Romanization adds controllability |
| MMS-LLaMA | 2025-03 | AVSR | Fuses modalities before tokenization to reduce compute |
| VALLR | 2025-03 | VSR | Predicts phonemes from a custom encoder into Llama |
| PV-ASR | 2025-07 | VSR | Combines custom encoders with lip-landmark features |

[^references]: See the “References” slides at the end.

</div>

---
layout: center
---

# Where Visual Speech Meets Language: VSP-LLM Framework for Efficient and Context-Aware Visual Speech Processing
[^VSPLLM]
[^VSPLLM]: [J. H. Yeo et al., “Where Visual Speech Meets Language: VSP-LLM Framework for Efficient and Context-Aware Visual Speech Processing,” May 14, 2024, arXiv: arXiv:2402.15151. doi: 10.48550/arXiv.2402.15151.](https://arxiv.org/abs/2402.15151)

---

## VSP-LLM: Problem Statement

- Can we leverage LLM capabilities for VSR?
- With limited training data, can an LLM-backed model still deliver strong performance?

---

## VSP-LLM: Contributions to VSR

1. First work to integrate visual speech modeling with an LLM, achieving state-of-the-art performance on VSR and VST
2. Groups consecutive frames based on feature similarity via k-means rather than fixed intervals

---

## VSP-LLM: Method

<img class="h-80 place-self-center" src="/vsr-llm/vsp-llm-x1.png">[^VSP-LLM]

[^VSP-LLM]: [J. H. Yeo et al., “Where Visual Speech Meets Language: VSP-LLM Framework for Efficient and Context-Aware Visual Speech Processing.”](https://arxiv.org/abs/2402.15151)

---

## VSP-LLM: Pseudocode

The model input consists of visual features, the frame counts per cluster ID, and instructions for the LLM.

```py
vsp_llm.generate({
  "source": {
    "audio": None,
    "video": torch.Tensor,
    "cluster_counts": [1, 3, 2, 1, 4, 1, 1, 1, 3, 1, 2],
    "text": some_instruction,
  },
  "padding_mask": torch.Tensor,
  "text_attn_mask": torch.Tensor,
})
```
<!-- TODO: Explain padding_mask and text_attn_mask -->

---

## Inspecting AV-HuBERT Embeddings

<img class="h-80 place-self-center" src="https://img.youtube.com/vi/3y3JY4iEamc/maxresdefault.jpg">[^AV-HuBERT]

[^AV-HuBERT]: [B. Shi et al., “Learning Audio-Visual Speech Representation by Masked Multimodal Cluster Prediction.”](https://arxiv.org/abs/2201.02184) Demonstration by [@xhiroga](https://www.youtube.com/watch?v=3y3JY4iEamc)

---

VSP-LLM versus other VSR methods. Using an LLM yields the best performance among self-supervised approaches and competitive averages compared with supervised methods.

<img class="h-80 place-self-center" src="/vsr-llm/vsp-llm-table1.png">[^VSP-LLM]

[^VSP-LLM]: [J. H. Yeo et al., “Where Visual Speech Meets Language: VSP-LLM Framework for Efficient and Context-Aware Visual Speech Processing.”](https://arxiv.org/abs/2402.15151)

<!-- VST performance omitted -->

<!-- Add implementation details if space allows -->

---

## VSP-LLM: Demo (LRS3)

<img class="h-80 place-self-center" src="https://img.youtube.com/vi/xN2htGRxC-M/maxresdefault.jpg">[^VSP-LLM]

[^VSP-LLM]: [Sally-SH, “VSP-LLM,” GitHub.](https://github.com/Sally-SH/VSP-LLM) Adapted by [@xhiroga](https://www.youtube.com/watch?v=xN2htGRxC-M)

---

## (Reprise) VSP-LLM: Demo (WebCam)

<img class="h-80 place-self-center" src="https://img.youtube.com/vi/ltW1nRTGCH0/maxresdefault.jpg">[^VSP-LLM]

[^VSP-LLM]: [Sally-SH, “VSP-LLM,” GitHub.](https://github.com/Sally-SH/VSP-LLM) Adapted by [@xhiroga](https://www.youtube.com/watch?v=ltW1nRTGCH0)

---

## VSP-LLM: Observations

- The raw capability of the LLM matters a lot
  - Because it uses Llama 2, the model can sometimes loop and repeat words
- Clustering AV-HuBERT embeddings and then averaging them feels redundant
  - If vectors are similar enough to cluster, why average them again?
  - Later work sometimes reuses cached prototypes instead
- High scores on standard datasets, but real-world usage (e.g., webcam recordings) still seems challenging

---
layout: center
---

# Zero-AVSR: Zero-Shot Audio-Visual Speech Recognition with LLMs by Learning Language-Agnostic Speech Representations[^ZeroAVSR]
[^ZeroAVSR]: [J. H. Yeo, M. Kim, C. W. Kim, S. Petridis, and Y. M. Ro, “Zero-AVSR: Zero-Shot Audio-Visual Speech Recognition with LLMs by Learning Language-Agnostic Speech Representations,” July 21, 2025, arXiv: arXiv:2503.06273. doi: 10.48550/arXiv.2503.06273.](https://arxiv.org/abs/2503.06273)

---

## Zero-AVSR: Problem Statement

- Conventional audio-visual models depend on the languages seen during training
- Existing multilingual audio-visual datasets cover only a handful of languages

---

mTEDx: Covers eight languages besides English.

Dataset page: <a href="https://www.openslr.org/100" target="_blank">OpenSLR #100</a>

<a href="https://www.openslr.org/100" target="_blank"><img class="h-100 place-self-center" src="/vsr-llm/openslr-mtedx.png"></a>[^mTEDx]

[^mTEDx]: [E. Salesky et al., “The Multilingual TEDx Corpus for Speech Recognition and Translation.”](https://arxiv.org/abs/2102.01757)

---

MuAViC: Covers nine languages including English. Composed of LRS3, mTEDx, and more.

Repository: <a href="https://github.com/facebookresearch/muavic" target="_blank">github.com/facebookresearch/muavic</a>

<a href="https://github.com/facebookresearch/muavic" target="_blank"><img class="h-80 place-self-center" src="/vsr-llm/github-muavic.png"></a>[^MuAViC]

[^MuAViC]: [M. Anwar et al., “MuAViC: A Multilingual Audio-Visual Corpus for Robust Speech Recognition and Robust Speech-to-Text Translation.”](https://github.com/facebookresearch/muavic)

<!-- TODO: Explain why hours are reported for training -->

---

## Zero-AVSR: Contributions

- Proposes the MARC dataset
- Demonstrates cross-lingual transfer potential with the AV-HuBERT & Llama architecture
- Improves controllability by inserting a romanization module

---
layout: two-cols-header
---

## Zero-AVSR: Method (MARC Dataset)

::left::

### Source Data

- LRS3 (433 hours, English, labeled)
- MuAViC (1,200 hours, 9 languages, labeled)
- VoxCeleb2 (2,442 hours, multilingual, unlabeled)
- AVSpeech (4,700 hours, multilingual, unlabeled)

::right::

### Proposed Romanized Transcripts

- der puls und der blutdruck steigen → d e r | p u l s | u n d | d e r | b l u t d r u c k | s t e i g e n |
- vielen dank → v i e l e n | d a n k |

---

## Zero-AVSR: Method (Cascaded Zero-AVSR)

<img class="h-80 place-self-center" src="/vsr-llm/zero-avsr-x1.png">[^Zero-AVSR]

[^Zero-AVSR]: [J. H. Yeo et al., “Zero-AVSR: Zero-Shot Audio-Visual Speech Recognition with LLMs by Learning Language-Agnostic Speech Representations.”](https://arxiv.org/abs/2503.06273)

---

Before implementing Cascaded Zero-AVSR, the authors benchmarked candidate LLMs. GPT-4o mini outperformed Llama in their experiments.

<img class="h-80 place-self-center" src="/vsr-llm/zero-avsr-table1.png">[^Zero-AVSR]

[^Zero-AVSR]: [J. H. Yeo et al., “Zero-AVSR: Zero-Shot Audio-Visual Speech Recognition with LLMs by Learning Language-Agnostic Speech Representations.”](https://arxiv.org/abs/2503.06273)

---

## Zero-AVSR: Method (Directly Integrated Zero-AVSR)

<img class="h-80 place-self-center" src="/vsr-llm/zero-avsr-x2.png">[^Zero-AVSR]

[^Zero-AVSR]: [J. H. Yeo et al., “Zero-AVSR: Zero-Shot Audio-Visual Speech Recognition with LLMs by Learning Language-Agnostic Speech Representations.”](https://arxiv.org/abs/2503.06273)

---

- Comparison between Zero-AVSR and other AVSR models. The top four rows use multilingual training, while the bottom three claim zero-shot inference in the target language.
- In reality, the romanization module and Llama pretraining already draw on multiple languages, so the definition of “zero-shot” deserves scrutiny.

<img class="h-60 place-self-center" src="/vsr-llm/zero-avsr-table2.png">[^Zero-AVSR]

[^Zero-AVSR]: [J. H. Yeo et al., “Zero-AVSR: Zero-Shot Audio-Visual Speech Recognition with LLMs by Learning Language-Agnostic Speech Representations.”](https://arxiv.org/abs/2503.06273)

<!-- Add implementation notes if possible -->

---

## Zero-AVSR: Demo (MuAViC)

<img class="h-80 place-self-center" src="https://img.youtube.com/vi/2jx0UJOOJ_A/maxresdefault.jpg">[^Zero-AVSR]

[^Zero-AVSR]: [JeongHun0716, “zero-avsr,” GitHub.](https://github.com/JeongHun0716/zero-avsr) Adapted by [@xhiroga](https://www.youtube.com/watch?v=2jx0UJOOJ_A)

---

## Zero-AVSR: Demo (WebCam)

<img class="h-80 place-self-center" src="https://img.youtube.com/vi/Mcu_1fmdMe4/maxresdefault.jpg">[^Zero-AVSR]

[^Zero-AVSR]: [JeongHun0716, “zero-avsr,” GitHub.](https://github.com/JeongHun0716/zero-avsr) Adapted by [@xhiroga](https://www.youtube.com/watch?v=Mcu_1fmdMe4)

---

## Zero-AVSR: Observations

- Reinforces how powerful AV-HuBERT and Llama already are
- Although the pipeline appears to romanize AV-HuBERT outputs before feeding Llama, the final architecture simply resamples AV-HuBERT embeddings
  - Romanization is therefore less central (though useful for development and operations)
- The biggest takeaway for me: multilingual dataset training significantly shifts AV-HuBERT weights, suggesting room to scale with longer training

---

## Zero-AVSR: More to Explore

https://zenn.dev/hiroga/articles/zero-avsr-eval

---

## Takeaways

- Lip reading (visual speech recognition) is increasingly adopting LLM-based architectures instead of bespoke decoders
- Researchers are proposing diverse strategies for encoding video signals
- Many works aim to improve controllability via phonemes or romanization

---

## References (VSR/AVSR + LLM) #1

- [J. H. Yeo et al., “Where Visual Speech Meets Language: VSP-LLM Framework for Efficient and Context-Aware Visual Speech Processing,” May 14, 2024, arXiv: arXiv:2402.15151. doi: 10.48550/arXiv.2402.15151.](https://arxiv.org/abs/2402.15151)
- [J. H. Yeo et al., “Personalized Lip Reading: Adapting to Your Unique Lip Movements with Vision and Language,” Jan. 01, 2025, arXiv: arXiv:2409.00986. doi: 10.48550/arXiv.2409.00986.](https://arxiv.org/abs/2409.00986)
- [U. Cappellazzo et al., “Large Language Models Are Strong Audio-Visual Speech Recognition Learners,” Mar. 07, 2025, arXiv: arXiv:2409.12319. doi: 10.48550/arXiv.2409.12319.](https://arxiv.org/abs/2409.12319)

---

## References (VSR/AVSR + LLM) #2

- [J. H. Yeo et al., “Zero-AVSR: Zero-Shot Audio-Visual Speech Recognition with LLMs by Learning Language-Agnostic Speech Representations,” July 21, 2025, arXiv: arXiv:2503.06273. doi: 10.48550/arXiv.2503.06273.](https://arxiv.org/abs/2503.06273)
- [J. H. Yeo et al., “MMS-LLaMA: Efficient LLM-based Audio-Visual Speech Recognition with Minimal Multimodal Speech Tokens,” June 05, 2025, arXiv: arXiv:2503.11315. doi: 10.48550/arXiv.2503.11315.](https://arxiv.org/abs/2503.11315)
- [M. Thomas et al., “VALLR: Visual ASR Language Model for Lip Reading,” Mar. 27, 2025, arXiv: arXiv:2503.21408. doi: 10.48550/arXiv.2503.21408.](https://arxiv.org/abs/2503.21408v1)
- [M. K. K. Teng et al., “Phoneme-Level Visual Speech Recognition via Point-Visual Fusion and Language Model Reconstruction,” July 25, 2025, arXiv: arXiv:2507.18863. doi: 10.48550/arXiv.2507.18863.](https://arxiv.org/abs/2507.18863)

---

## References (Base Models, Datasets, Surveys)

- [B. Shi et al., “Learning Audio-Visual Speech Representation by Masked Multimodal Cluster Prediction,” Mar. 13, 2022, arXiv: arXiv:2201.02184. doi: 10.48550/arXiv.2201.02184.](https://arxiv.org/abs/2201.02184)
- [E. Salesky et al., “The Multilingual TEDx Corpus for Speech Recognition and Translation,” June 15, 2021, arXiv: arXiv:2102.01757. doi: 10.48550/arXiv.2102.01757.](https://arxiv.org/abs/2102.01757)
- [M. Anwar et al., “MuAViC: A Multilingual Audio-Visual Corpus for Robust Speech Recognition and Robust Speech-to-Text Translation,” Mar. 07, 2023, arXiv: arXiv:2303.00628. doi: 10.48550/arXiv.2303.00628.](https://arxiv.org/abs/2303.00628)
- [K. Rezaee and M. Yeganeh, “Automatic Visual Lip Reading: A Comparative Review of Machine-Learning Approaches,” Results in Engineering, p. 107171, Sept. 2025, doi: 10.1016/j.rineng.2025.107171.](https://www.sciencedirect.com/science/article/pii/S2590123025032268)
